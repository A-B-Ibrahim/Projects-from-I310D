# -*- coding: utf-8 -*-
"""abi276_Data_Curation_and_Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xDSpK2UGKKIEAr207JwTmPSj7TIVe8RR

#Purpose
This is an assignment I'm completing for my I310D class in data curration and analysis. I chose to use the meta wiki page that describes about wikipedia because it has a large number of data points and many different attributes in each row, making it good for show casing data analysis skills.

I'm using the beautifulsoup library as it is what I am most familarly with from class to webscrape.
"""

#install the API (beautiful soup) for web scrapping
from bs4 import BeautifulSoup
import pandas
import requests

#scrap wikipedia page of the U.N. presidents
opened_webpage = requests.get("https://meta.wikimedia.org/wiki/List_of_Wikipedias/Table")
bs = BeautifulSoup(opened_webpage.content, "html.parser")

"""#Curration:
The webpage only has two tables so instead of looking specificly for the table with beautiful soup, I start by organizing the "tr" text. I seperate each of the "tr" into its own group and then further subdivide that group into the different parts of the table. The infomration in not taken out of order when subdivided so that the 1st position in each of the "tr" groups correpsonds to the same colomn. While I'm doing this, I'm also cleaning the data, removing things like empty space, commas, and turning the numbers into an operable form (float). Once this is done, I have all the raw data I need from the webpage.

Note: The second (unwanted) table is removed in the next step.
"""

#find and save all tables from the wikipedia page
raw_data = []
trs = bs.find_all("tr")
for tr in trs:
  raw_line=[]
  for row in tr:
    line = row.text
    #clean data
    line=line.replace("\n","")
    try:
      line=line.replace(",","")
      line=float(line)
    except:
      pass
    raw_line.append(line)
    #clean data
  raw_line=list(filter(None, raw_line))
  #create list of different wars including start, finish, name of conflict, winner, and loser
  raw_data.append(raw_line)
#clean data (remove Nan entries)
raw_data=list(filter(None, raw_data))

"""#Transformation:
First, I manually name all the colomns based on what is used in the original table (through the use of a dict). I then go through each "tr" group and put the correpsonding data into it's appropriate colomn. I can do this because everything is orderly on the orginal table, so as I didn't take it out of order when currating the data, I just need to put things back in a set order and have the colomns headers also lined up in that order. Here, we get rid of the second unwanted table by going in reverse order. The second table has less colomns than the first one (the one we want) so when you try to go in reverse, the second table fails as it doesn't have an item corresponding to the final colomn. This cases it to fail and thus skip over the table. The data is transformed during this process.

Afterwards, the panda library is used to convert the transformed into a neat data frame and a csv file is made for easier acessability.

"""

#transform raw data into a data frame
#name coloumns
transformed_data={"Number":[],"Language":[],"Language(local)":[],"Wiki":[],"Articles":[],"All pages":[],"Edits":[],"Admins":[],"Users":[],"Active Users":[],"Files":[],"Depth":[]}
#sort data into coloums
for entry in raw_data[1:]:
  try:
    transformed_data["Depth"].append(entry[11])
    transformed_data["Files"].append(entry[10])
    transformed_data["Active Users"].append(entry[9])
    transformed_data["Users"].append(entry[8])
    transformed_data["Admins"].append(entry[7])
    transformed_data["Edits"].append(entry[6])
    transformed_data["All pages"].append(entry[5])
    transformed_data["Articles"].append(entry[4])
    transformed_data["Wiki"].append(entry[3])
    transformed_data["Language(local)"].append(entry[2])
    transformed_data["Language"].append(entry[1])
    transformed_data["Number"].append(entry[0])
  #remove any non conforming data points
  except:
    continue

# create data frame
transformed_data=pandas.DataFrame(transformed_data)
# create csv file of data frame
transformed_data.to_csv("List of Wikipedias.csv")

"""
#Hypothesis:
There will be a difference (in correlation) between regular and active users when in relation to other stats.

#Process
I used the matplotlib library from python to make a number of scatter plots to showing the correlation between other stats and active/regular users. The plots in red are the active user while the blue are regular. They are in pairs (vertically) with the correpsonding active and regular user graphs for the same additional stat being together (active comes first followed by regular in all pairs). The user is always the x-axis for consistency. Each plot has its axises limited due to some outliers making it difficult to see the plot trends clearly. The active and regular user axis is limited by the same amount across all comparisons for consistency while the other stat is limit to the same degree in both comparisons. Note: The active users axis is limited by more than the regular user axis as the number of regular users is far greater."""

#compare how the relationship to other stats between the regular user count and active user
import matplotlib.pyplot as plt
#correlation of Users/active users and total # of edits
from scipy.stats import pearsonr
correlation=pearsonr(transformed_data["Active Users"],transformed_data["Edits"])
print(correlation)
plt.scatter(transformed_data["Active Users"],transformed_data["Edits"],color="red")
plt.xlabel("Active Users")
plt.xlim([0,20000])
plt.ylabel("Edits (in hundred millions)")
plt.ylim([0,300000000])
plt.title("Active Users vs Number of Edits")
plt.show()

correlation=pearsonr(transformed_data["Users"],transformed_data["Edits"])
print(correlation)
plt.scatter(transformed_data["Users"],transformed_data["Edits"])
plt.xlabel("Users")
plt.xlim([0,6000000])
plt.ylabel("Edits (in hundred millions)")
plt.ylim([0,300000000])
plt.title("Users vs Number of Edits")
plt.show()

#correlation of Users/active users and total # of articles
correlation=pearsonr(transformed_data["Active Users"],transformed_data["Articles"])
print(correlation)
plt.scatter(transformed_data["Active Users"],transformed_data["Articles"],color="red")
plt.xlabel("Active Users")
plt.xlim([0,20000])
plt.ylabel("total # of articles")
plt.ylim([0,4000000])
plt.title("Active Users vs Articles")
plt.show()

correlation=pearsonr(transformed_data["Users"],transformed_data["Articles"])
print(correlation)
plt.scatter(transformed_data["Users"],transformed_data["Articles"])
plt.xlabel("Users")
plt.xlim([0,6000000])
plt.ylabel("total # of articles")
plt.ylim([0,4000000])
plt.title("Users vs Articles")
plt.show()

#correlation of Users/active users and total # of All pages
correlation=pearsonr(transformed_data["Active Users"],transformed_data["All pages"])
print(correlation)
plt.scatter(transformed_data["Active Users"],transformed_data["All pages"],color="red")
plt.xlabel("Active Users")
plt.xlim([0,20000])
plt.ylabel("Total # of All pages")
plt.ylim([0,20000000])
plt.title("Active Users vs All pages")
plt.show()

correlation=pearsonr(transformed_data["Users"],transformed_data["All pages"])
print(correlation)
plt.scatter(transformed_data["Users"],transformed_data["All pages"])
plt.xlabel("Users")
plt.xlim([0,6000000])
plt.ylabel("Total # of All pages")
plt.ylim([0,20000000])
plt.title("Users vs All pages")
plt.show()

"""#Insights
My original hypothesis was that there would be a notable difference between active users and the regular users however, that didn't end up being the case. In every example shown above, there is no clear difference between the two in how they relate to other stats.
"""